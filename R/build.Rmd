---
title: "Data Mining Homework 2"
author: "Parker Gauthier"
date: "2/28/2022"
output: md_document
---

```{r include=FALSE}
if (!("librarian" %in% rownames(utils::installed.packages()))) {
  utils::install.packages("librarian")
}

librarian::shelf( 
  cran_repo = "https://cran.microsoft.com/",
  ask = FALSE,
  stats, 
  here,
  kableExtra,
  ggthemes,
  tidyverse,
  lubridate,
  haven,
  lmtest,
  gganimate,
  gapminder,
  stargazer,
  snakecase,
  mosaic,
  dplyr,
  esquisse,
  plotly,
  modelr,
  rsample,
  caret,
  foreach,
  parallel,
  gamlr,
  glmnet
)

here::i_am("R/include.R")
```


## 1.) Visualization
```{r include=FALSE}
#Reading in the Data
capmetro = read.csv(here("data/capmetro_UT.csv"))
```
### Bus Boardings vs Time
```{r Average Boardings by Hour (1), echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}

capmetro = mutate(capmetro,
               day_of_week = factor(day_of_week,
                 levels=c("Mon", "Tue", "Wed","Thu", "Fri", "Sat", "Sun")),
               month = factor(month,
                 levels=c("Sep", "Oct","Nov")))

capmetro %>% 
  group_by(day_of_week, hour_of_day, month) %>% 
  summarize(mean = mean(boarding)) %>% 
  ggplot(aes(x = hour_of_day, y = mean, color = month)) +
  geom_line(size=1.1, alpha = .6)+
  facet_wrap(~day_of_week) +
  labs(title = "Capital Metro's Bus Boardings", 
       subtitle = "How do  Boarding Averages Vary by Time-of-Day, Month, and Day of the Week?", 
       x = "Hour of the Day", 
       y = "Average Boardings", 
       color = "Month:") +
  theme_economist() +
  scale_color_brewer(palette = "Set1")
  
```
```
```
  The figure above gives us some useful insights into the demand for Capital Metro's public bus line at different points in time.  Perhaps the most obvious takeaway is that demand is significantly less on the weekends than on the weekdays, with Saturday's and Sunday's demand being quite meager compared to that of Monday-Friday.  Moreover, peak demand on Monday through Friday is similar from day to day, having a steady increase from the morning hours until early evening. This suggests that the bus line is mostly used by students who have classes in the afternoon and evening during the week. Furthermore, the growth in boardings later in the day could suggest that most students stay on campus until this time, despite when their classes start.
```
```
    Additionally, there are some interesting trends unique to the monthly data.  It would appear that in September, boardings on Mondays are fewer than any day of the week.  This could be the result of students taking part in recreational weekend activities since September is early in the semester and they have less pressure from midterms and final exams.  Another trend is that average boardings in November on Wednesday, Thursday, and Friday are lower than any other month.  This could be the result of late semester burnout as students may wish to go home earlier to rest later in the week.  Another explanation for these trends are when holidays take place in these months.  September has Labor Day which always falls on a Monday, and November has Thanksgiving Break which starts on a Wednesday and goes through Friday.
    
    
### Bus Boardings vs Temperature
```{r Boardings vs Temp (1), echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}

capmetro %>%
  mutate(isWeekend = ifelse(day_of_week == "Sat" | day_of_week == "Sun", yes = TRUE, no = FALSE)) %>% 
  ggplot(aes(x=temperature, y= boarding, color=isWeekend)) +
  geom_point(alpha = .4) +
  facet_wrap(~hour_of_day) +
  labs(x = "Temperature (Degrees Fahrenheit)", 
       y = "Number of People Boarding", 
       title = "How Does Temperature Affect the Number of Boardings?", 
       caption = "**Each point represents the number of boardings in one 15-minute interval",
       color = "Weekday vs Weekend:") +
  scale_color_brewer(palette = "Set1", 
                     labels = c("Weekday", "Weekend")) +
  theme_economist()
```
```
```
  Our next figure above depicts the relationship between the number of boardings and temperature.  As indicated in the graph, each point represents the number of boardings in a 15-minute interval, and the plots are faceted by the hour of the day.  Again, we can see that demand is much greater during the week than on the weekends, consistent with our first figure.  More importantly, we can see that there is no apparent relationship between temperature and the number of boardings in the day.  Boardings appear to be affected more by the time of the day, not how hot or cold it is outside.
  
  

## 2.) Saratoga House Prices

  There are several ways to build an optimal pricing model, but depending on the data we are analyzing, some strategies may prove to be better than others. The analysis below aims to compare the performance of an optimized linear model to a K-nearest neighbors (KNN) model.  This will be done by separating data from Saratoga housing prices into a training set for building our model and a testing set for assessing our model's out-of-sample performance.  We will then build the best linear model we can and see how it stacks up to KNN.
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}
#Dividing the Data into an intital train and test split
saratoga_split = initial_split(SaratogaHouses, prop = 0.8) 
saratoga_train = training(saratoga_split) 
saratoga_test = testing(saratoga_split) 
```
  Lets begin by calculating the out-of-sample, Root Mean Squared Error (RMSE) for two different linear models.  The first will be "built by hand" as the coefficients will be selected based off arbitrary assumptions, made by yours truly, of their ability to predict price.  These coefficients will include age of the home, land value, bedrooms, and bathrooms among others and some interaction effects.  The other will be constructed using the Lasso regression method. Finally, to ensure the validity of the best performing model, we will loop through this process 30 times and calculate the average RMSE for each.
```{r include = FALSE, echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}


###################BUILDING A LINEAR MODEL#################  


##This will be our baseline.  It is an improved model relative to the medium model we used in class.  We will use this as a start to stepwise selection
lm_ = lm(price ~ age + landValue + bedrooms + fireplaces + bathrooms + rooms + centralAir + lotSize + livingArea + heating + lotSize*bedrooms + lotSize*bathrooms + lotSize*rooms + livingArea*lotSize + livingArea*rooms, data = saratoga_train)

```
*The average RMSE for our 'hand-built' model:
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}
rmse_og = foreach(i = 1:30, .combine='c') %do% {
  saratoga_split_loop = initial_split(SaratogaHouses, prop = 0.8) 
  saratoga_train_loop = training(saratoga_split_loop) 
  saratoga_test_loop = testing(saratoga_split_loop)
  lm_loop = lm(price ~ age + landValue + bedrooms + fireplaces + bathrooms + rooms + centralAir + lotSize + livingArea + heating + lotSize*bedrooms + lotSize*bathrooms + lotSize*rooms + livingArea*lotSize + livingArea*rooms, data = saratoga_train_loop)
  modelr::rmse(lm_loop, data=saratoga_test_loop)
} %>%
  mean() %>%
  round(2)

rmse_og
```

*Cross validated RMSE for Lasso Regression
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}
# model matricies
rmse_lasso = foreach(i = 1:30, .combine='c') %do% {
  saratoga_split_loop = initial_split(SaratogaHouses, prop = 0.8) 
  saratoga_train_loop = training(saratoga_split_loop) 
  saratoga_test_loop = testing(saratoga_split_loop)
  
  sara_x = model.matrix(price ~ . -1, data = saratoga_train)
  sara_y = saratoga_train$price

  # cross validated
  sara_cv = cv.glmnet(sara_x, sara_y, alpha = 1)

  best_lambda = sara_cv$lambda.min

  sara_x_test = model.matrix(price~.-1, data = saratoga_test)

  best_sara = glmnet(sara_x, sara_y, alpha = 1, lambda = best_lambda)

  sara_pred = predict(best_sara, s=  best_lambda, newx = sara_x_test)

  RMSE(sara_pred, saratoga_test$price)
} %>%
  mean() %>%
  round(2)

rmse_lasso
```


  Now that we have some scores for different builds of linear models, let us see how these compare to our KNN model.

  For our KNN model, we will optimize k and will initiate a similar loop to determine the average out-of-sample performance of our model.

*The RMSE for the KNN model:
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}


##########################BUILDING A KNN MODEL#####################


## constructing the feature matrices
xtrain = model.matrix(~ . - 1, data = saratoga_train)
xtest = model.matrix(~ .- 1, data = saratoga_test)

#train/test responces
ytrain = saratoga_train$price
ytest = saratoga_test$price

#rescaling by stdv to get z-scores in our feature matricies
scale_train = apply(xtrain, 2, sd)
xtilde_train = scale(xtrain, scale = scale_train)
xtilde_test = scale(xtest, scale = scale_train)

#Running KNN
### Maybe use old code to find the optimal k???
knn_ = knnreg(ytrain~xtilde_train, data = saratoga_train, k = 15)

xval = c(1:30)

rmse_knn = foreach(i = 1:30, .combine='c') %do% {
  #split data
  saratoga_split_loop = initial_split(SaratogaHouses, prop = 0.8) 
  saratoga_train_loop = training(saratoga_split_loop) 
  saratoga_test_loop = testing(saratoga_split_loop)
  
  #feature matricies
  xtrain_loop = model.matrix(~ . - 1, data = saratoga_train_loop)
  xtest_loop = model.matrix(~ .- 1, data = saratoga_test_loop)
  
  #train/test responces
  ytrain_loop = saratoga_train_loop$price
  ytest_loop = saratoga_test_loop$price
  
  #rescaling
  scale_train_loop = apply(xtrain_loop, 2, sd)
  xtilde_train_loop = scale(xtrain_loop, scale = scale_train_loop)
  xtilde_test_loop = scale(xtest_loop, scale = scale_train_loop)
  
  #making data frames for regression
  xtilde_train_loop = data.frame(xtilde_train_loop) %>%
  mutate(price = c(ytrain_loop))

  xtilde_test_loop = data.frame(xtilde_test_loop) %>%
  mutate(price = c(ytest_loop))
  
  #finding optimal k
  rmse_loop = foreach(j = 1:30, .combine='c') %do%{
    knn_inner = knnreg(price ~ ., data = xtilde_train_loop, k = j)
    modelr::rmse(knn_inner, data = xtilde_test_loop)
  }
  
  rmse_frame = data.frame(rmse_loop, xval)
  
  k_frame = rmse_frame %>%
    filter(rmse_loop!=0) %>%
    arrange(rmse_loop) %>%
    head(1) %>%
    select(xval)
  
  optimal = k_frame[1,1]
  
  #running KNN
  knn_loop = knnreg(price ~ ., data = xtilde_train_loop, k = optimal)
  modelr::rmse(knn_loop, data = xtilde_test_loop)
} %>%
  mean()

rmse_knn
```


  It would appear that our linear models, particularly our Lasso model outperforms our KNN model. To optimally predict price, we should build a linear model using Lasso regularization. 


## 3.) Classification and retrospective sampling
```{r include=FALSE}
credit = read.csv(here("data/german_credit.csv"))
```

### Ploting probability of default

  To begin this analysis, we will take a look at the likelihood that one will default based solely on their credit history.  Particularly, we will be looking at the probability that one defaults given their history:
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}

counts = addmargins(table(credit$Default, credit$history))

good_p = counts[2,1]/counts[3,1]
poor_p = counts[2,2]/counts[3,2]
terrible_p = counts[2,3]/counts[3,3]

frame_ = data.frame(prob = c(good_p,poor_p,terrible_p), history = c("Good","Poor","Terrible"))

frame_ %>% 
  ggplot(aes(x = history, y = prob, fill = history)) +
  geom_col() +
  labs(x = "Credit History", y = "Default Probability", title = "Probability of Default based on Credit History") +
  scale_fill_discrete() +
  scale_y_continuous(labels=scales::percent) +
  theme_igray() +
  theme(legend.position = "none")
```
```
```

  The plot above suggests that the probability of defaulting goes DOWN with worse credit history.  It does not take much to infer that something is amiss with our data set.


### Making a predictive model
  The red-flags from the plot above may be exaggerated by building a predictive model.  We will begin this process by splitting the data into a training and testing set.  We will then use a logistic model to help us determine the probability that one will default on their loan.
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}
##Splitting data to test out of sample preformance
credit_split = initial_split(credit, prop = .8)
credit_train = training(credit_split)
credit_test = testing(credit_split)
```

The coefficients for our logistic model:
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}

##building a predicitive model and checking its performance 
lm_credit = glm(Default ~ duration + amount + installment + age + history + purpose + foreign, data=credit_train, family = "binomial")

coef(lm_credit)
```

  As seen above, holding all else fixed, the magnitude of the coefficients do not explicitly tell us the odds that one will default given credit history. However, the signs suggest that poor and terrible history has less of a chance of defaulting than good.

  These results show that the method for collecting this data is quite clearly flawed.  This data set seems to suggest that those with 'terrible' credit history are just as likely to default on a loan as those with 'good' credit history.  This does not make any intuitive sense and is likely the result of improper sampling.  Moreover, it dismisses any predictive power of a logistic model to data outside of this sample. If the bank tried to build a model using random sampling, they would likely be able to build a better predictive model regarding rates of default.


## 4.) Children and Hotel Reservations
```{r include=FALSE}
hotels_dev = read.csv(here("data/hotels_dev.csv"))
hotels_val = read.csv(here("data/hotels_val.csv"))
```

```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}
hotels_dev = hotels_dev %>% mutate(hotels_dev, arrival_date = ymd(arrival_date)) %>% filter(reserved_room_type != "L")
hotels_val = mutate(hotels_val, arrival_date = ymd(arrival_date)) %>% filter(reserved_room_type != "L")


dev_split = initial_split(hotels_dev, prop = .8)
dev_train = training(dev_split)
dev_test = testing(dev_split)
```

#### Baseline 1
Below are the predictions on the testing set. This will be used to measure out-of-sample performance. We can see that it never predicted children would be on the itinerary.
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}
BaseLine1 = glm(children ~ (market_segment + adults + customer_type + is_repeated_guest), data = dev_train, family = "binomial")

pred_base1 = predict(BaseLine1, dev_test, type = 'response')

yhat_base1 = ifelse(pred_base1 > 0.5, 1, 0)

confusion_base1 = table(Actual = dev_test$children, Predictions = yhat_base1)

confusion_base1

```
*Percentage of out-of-sample correct classifications:
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}

 round(sum(diag(confusion_base1))/sum(confusion_base1) * 100, 2)

```

#### Baseline 2
Using the same methodology, the second baseline model shows slightly better results than our previous model:
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}
BaseLine2 = glm(children ~ . - arrival_date, data = dev_train, family = 'binomial')

pred_base2 = predict(BaseLine2, dev_test, type = 'response')

yhat_base2 = ifelse(pred_base2 > 0.5, 1, 0)

confusion_base2 = table(Actual = dev_test$children, Predictions = yhat_base2)

confusion_base2

```
*Percentage of out-of-sample correct classifications:
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}

 round(sum(diag(confusion_base2))/sum(confusion_base2) * 100, 2)

```

#### Best linear model
Finally, we see that our best linear model, constructed using Lasso regression, has similar results to Baseline 2:
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}


x = model.matrix(children ~ . - 1, data = dev_train)
y = dev_train$children

x_test = model.matrix(children~. -1, data = dev_test)

dev_lasso_cv = cv.glmnet(x, y, nfolds = 10, alpha = 1)

best_lambda = dev_lasso_cv$lambda.min

dev_best = glmnet(x,y,alpha=1,lambda = best_lambda)

dev_best_pred = predict(dev_best, s = best_lambda, newx = x_test)

yhat_best = ifelse(dev_best_pred > 0.5, 1 , 0)

confusion_best = table(Actual = dev_test$children, Predictions = yhat_best)

confusion_best
```
*Percentage of out-of-sample correct classifications:
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}

round(sum(diag(confusion_best))/sum(confusion_best) * 100, 2)

```




### Model Validation: Step 1
We will begin this process by constructing a confusion matrix of Actual vs Predicted values:
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}

x_val1 = model.matrix(children~.-1, data = hotels_val)

dev_val1_pred = predict(dev_best, s = best_lambda, newx = x_val1)

yhat_val1 = ifelse(dev_val1_pred > 0.5 , 1, 0)


confusion_val1 = table(Actual=hotels_val$children, Predictions = yhat_val1)

confusion_val1
```
*Percentage of out-of-sample correct classifications:
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}
round(sum(diag(confusion_val1))/sum(confusion_val1) * 100, 2)

```


We can see from the results above that our model performed well with the new data.

To drive this home, we will plot our results on a ROC curve by plotting the True Positive Rate against the False Positive Rate for different thresholds of t.

```{r ROC curve (4), echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}

TPR_t = foreach(i = 1:90, .combine = 'c') %do% {
  yhat_val_loop = ifelse(dev_val1_pred > (i/100) , 1, 0)
  confusion_val_loop = table(Actual=hotels_val$children, Predictions = yhat_val_loop)
  confusion_val_loop[2,2]/(confusion_val_loop[2,1] + confusion_val_loop[2,2])
}

FPR_t = foreach(i = 1:90, .combine = 'c') %do% {
  yhat_val_loop = ifelse(dev_val1_pred > (i/100) , 1, 0)
  confusion_val_loop = table(Actual=hotels_val$children, Predictions = yhat_val_loop)
  confusion_val_loop[1,2]/(confusion_val_loop[1,2] + confusion_val_loop[1,1])
}


ROC_data = data.frame(TPR_t,FPR_t)

ROC_data %>%
  ggplot(aes(x = FPR_t, y = TPR_t,)) +
  geom_line(color = "red") +
  labs(x = "FPR",
       y = "TPR",
       title = "ROC Curve For Our Best Linear Model") +
  theme_economist_white()
```

### Model Validation: Step 2
```{r Folds(4), echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}
n = nrow(hotels_val)
Kfolds = 20

hotels_val = hotels_val %>% mutate(fold_id = rep(1:Kfolds, length = n)%>%sample)

child_count_hat = c()
child_count_actual = c()

for(i in 1:Kfolds) {
  hotels_fold = hotels_val %>% filter(fold_id == i)
  preds = predict(BaseLine2, hotels_fold, type = "response")
  child_hat = sum(preds)
  child_count_hat = c(child_count_hat, child_hat)
  child_actual=sum(hotels_fold$children)
  child_count_actual=c(child_count_actual, child_actual)
}

fold = c(1:20)

fold_frame = data.frame(fold,child_count_hat,child_count_actual)


fold_frame %>% 
  ggplot(aes(x = fold)) +
  geom_col(aes(y = child_count_actual, color = child_count_actual), color = "blue", alpha = .7) +
  geom_col(aes(y = child_count_hat, color = child_count_hat), color = "red", alpha = .7) + 
  geom_point(aes(y = child_count_actual), color = "blue", size = 3.5) +
  geom_point(aes(y = child_count_hat), color = "red", size = 3.5) +
  labs(title = "Model Performance Over 20 folds", x = "Fold ID", y = "Families with Children", caption = "Blue: Actual, Red: Predicted") +
  guides(color = guide_legend(title = "Legend"))

```

For 20 folds of the data in hotels_val, the graph above shows the predicted vs the actual values of the number of families that will have children with them.  We can see that the model used here tends to over predict the number of children in each fold, but it seems that the predicted values are quite close to the actual values.

